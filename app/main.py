"""
–í–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –Ω–∞ Streamlit —Å —Ç—Ä–µ–º—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏:
1. Overview - –æ–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
2. Data - –ø—Ä–æ—Å–º–æ—Ç—Ä –¥–∞–Ω–Ω—ã—Ö
3. Functional Core - –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –Ω–∞—à–∏—Ö —Ñ—É–Ω–∫—Ü–∏–π (–≤–∫–ª—é—á–∞—è map/filter/reduce)
"""

from pathlib import Path
from functools import reduce
import json
import streamlit as st
import sys, os

# –µ—Å–ª–∏ —Ö–æ—á–µ—à—å —É–±–∏—Ä–∞—Ç—å —ç—Ç–æ—Ç hack ‚Äî —É—Å—Ç–∞–Ω–æ–≤–∏ –ø–∞–∫–µ—Ç –∫–∞–∫ –º–æ–¥—É–ª—å –∏–ª–∏ –∑–∞–ø—É—Å–∫–∞–π –∏–∑ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.domain import Document, User, Submission, Rule
from core.transforms import normalize, tokenize, ngrams, jaccard


@st.cache_data
def load_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ data/seed.json (–ø—É—Ç—å —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ app/)"""
    data_path = Path(__file__).parent.parent / "data" / "seed.json"
    if not data_path.exists():
        raise FileNotFoundError(f"{data_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
    raw = json.loads(data_path.read_text(encoding="utf-8"))

    documents = tuple(Document(**doc) for doc in raw.get("documents", []))
    users = tuple(User(**user) for user in raw.get("users", []))
    submissions = tuple(Submission(**sub) for sub in raw.get("submissions", []))
    rules = tuple(Rule(**rule) for rule in raw.get("rules", []))

    return documents, users, submissions, rules, raw


def main():
    st.set_page_config(
        page_title="üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç–∏",
        page_icon="üìù",
        layout="wide",
    )

    with st.sidebar:
        st.title("üìù –ú–µ–Ω—é –Ω–∞–≤–∏–≥–∞—Ü–∏–∏")
        page = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—É:", ["Overview", "Data", "Functional Core"])
        st.markdown("---")
        st.markdown("**–õ–∞–±–∞ ‚Ññ1**: –ß–∏—Å—Ç—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ + –Ω–µ–∏–∑–º–µ–Ω—è–µ–º–æ—Å—Ç—å + HOF")

    # –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    try:
        documents, users, submissions, rules, raw = load_data()
    except FileNotFoundError as e:
        st.error("‚ùå –§–∞–π–ª data/seed.json –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        st.info("–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, python3 1.py) –∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ.")
        st.stop()

    # ========== Overview ==========
    if page == "Overview":
        st.title("üìä –û–±–∑–æ—Ä —Å–∏—Å—Ç–µ–º—ã")
        # reduce: —Å—É–º–º–∏—Ä—É–µ–º –¥–ª–∏–Ω—ã
        total_doc_chars = reduce(lambda acc, d: acc + len(d.text), documents, 0)
        total_sub_chars = reduce(lambda acc, s: acc + len(s.text), submissions, 0)

        avg_doc_length = total_doc_chars / len(documents) if documents else 0
        avg_sub_length = total_sub_chars / len(submissions) if submissions else 0

        c1, c2, c3, c4 = st.columns(4)
        c1.metric("üìö –î–æ–∫—É–º–µ–Ω—Ç–æ–≤", len(documents))
        c2.metric("üë• –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π", len(users))
        c3.metric("üìù –ü—Ä–æ–≤–µ—Ä–æ–∫", len(submissions))
        c4.metric("‚öôÔ∏è –ü—Ä–∞–≤–∏–ª", len(rules))

        st.markdown("---")
        c5, c6 = st.columns(2)
        c5.metric("üìè –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞", f"{avg_doc_length:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
        c6.metric("üìè –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏", f"{avg_sub_length:.0f} —Å–∏–º–≤–æ–ª–æ–≤")

    # ========== Data ==========
    elif page == "Data":
        st.title("üìã –ü—Ä–æ—Å–º–æ—Ç—Ä –¥–∞–Ω–Ω—ã—Ö")
        tab1, tab2, tab3, tab4 = st.tabs(["üìö –î–æ–∫—É–º–µ–Ω—Ç—ã", "üë• –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏", "üìù –ü—Ä–æ–≤–µ—Ä–∫–∏", "‚öôÔ∏è –ü—Ä–∞–≤–∏–ª–∞"])

        with tab1:
            st.subheader("–≠—Ç–∞–ª–æ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã")
            for doc in documents[:10]:
                with st.expander(f"üìÑ {doc.title} (–∞–≤—Ç–æ—Ä: {doc.author})"):
                    st.write(f"**ID:** {doc.id}")
                    st.write(f"**–í—Ä–µ–º—è:** {doc.ts}")
                    st.write(f"**–¢–µ–∫—Å—Ç:** {doc.text}")

            if len(documents) > 10:
                st.info(f"–ü–æ–∫–∞–∑–∞–Ω—ã –ø–µ—Ä–≤—ã–µ 10 –∏–∑ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

        with tab2:
            st.subheader("–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ —Å–∏—Å—Ç–µ–º—ã")
            # map: —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤—ã–≤–æ–¥ –∏–º–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
            user_names = list(map(lambda u: f"üë§ {u.name} (ID: {u.id})", users))
            for name in user_names:
                st.write(name)

        with tab3:
            st.subheader("–ü—Ä–æ–≤–µ—Ä–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤")
            for sub in submissions[:10]:
                with st.expander(f"üìù {sub.id} ‚Äî {sub.user_id}"):
                    st.write(f"**–í—Ä–µ–º—è:** {sub.ts}")
                    st.write(sub.text)

        with tab4:
            st.subheader("–ü—Ä–∞–≤–∏–ª–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏")
            for rule in rules:
                st.write(f"‚öôÔ∏è **{rule.kind}**: {rule.payload}")

    # ========== Functional Core ==========
    elif page == "Functional Core":
        st.title("‚öôÔ∏è –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ñ—É–Ω–∫—Ü–∏–π")
        st.markdown("–¢—É—Ç –º—ã –ø–æ–∫–∞–∑—ã–≤–∞–µ–º `normalize`, `tokenize`, `ngrams`, `jaccard` –∏ HOF (map/filter/reduce).")

        test_text = st.text_area(
            "–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏:",
            value="–ü—Ä–∏–≤–µ—Ç, –ú–∏—Ä! –≠—Ç–æ —Ç–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ñ—É–Ω–∫—Ü–∏–π.",
            height=120,
        )

        ngram_n = st.slider("–†–∞–∑–º–µ—Ä n-–≥—Ä–∞–º–º—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è", 1, 5, 3)

        if test_text is not None:
            st.markdown("---")
            st.subheader("üßπ normalize / tokenize / ngrams")
            normalized = normalize(test_text)
            tokens = tokenize(normalized)
            gram = ngrams(tokens, n=ngram_n)

            st.write("**normalize:**", normalized)
            st.write("**tokenize:**", tokens)
            st.write(f"**{ngram_n}-grams:**", gram)

            st.markdown("---")
            st.subheader("üî¨ map / filter / reduce (–¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è)")

            # map: –¥–ª–∏–Ω—ã —Ç–æ–∫–µ–Ω–æ–≤
            token_lengths = list(map(len, tokens))
            # filter: —Ç–æ–∫–µ–Ω—ã –¥–ª–∏–Ω–µ–µ 2 —Å–∏–º–≤–æ–ª–æ–≤
            filtered_tokens = list(filter(lambda t: len(t) > 2, tokens))
            # reduce: —Å—É–º–º–∞ –¥–ª–∏–Ω
            total_chars = reduce(lambda a, b: a + b, token_lengths, 0)

            st.write("map(len, tokens) ->", token_lengths)
            st.write("filter(len>2) ->", filtered_tokens)
            st.write("reduce(sum lengths) ->", total_chars)

            st.markdown("---")
            st.subheader("üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ (–ø–æ n-–≥—Ä–∞–º–º–∞–º)")

            # —Å—Ö–æ–¥—Å—Ç–≤–æ —Å –ø–µ—Ä–≤—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –∏ —Ç–æ–ø-5 –ø–æ—Ö–æ–∂–∏—Ö
            sims = []
            for doc in documents:
                doc_tokens = tokenize(normalize(doc.text))
                doc_grams = ngrams(doc_tokens, n=ngram_n)
                sim = jaccard(gram, doc_grams)
                sims.append((sim, doc))

            sims_sorted = sorted(sims, key=lambda x: x[0], reverse=True)
            top = sims_sorted[:5]

            st.write("–¢–æ–ø-5 –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–ø–æ n-–≥—Ä–∞–º–º–∞–º):")
            for sim, doc in top:
                st.write(f"- {doc.id} ‚Äî **{doc.title}** (–∞–≤—Ç–æ—Ä: {doc.author}) ‚Äî –ø–æ—Ö–æ–∂–µ—Å—Ç—å: {sim:.3f}")

            # –ø–æ—Ä–æ–≥–∏ –∏–∑ –ø—Ä–∞–≤–∏–ª (–µ—Å–ª–∏ –µ—Å—Ç—å)
            thr_rule = next((r for r in rules if r.kind == "similarity_threshold"), None)
            if thr_rule:
                thr = thr_rule.payload.get("threshold", 0.0)
                st.info(f"–ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ –ø–æ –ø—Ä–∞–≤–∏–ª—É: {thr} (–∏–∑ rules)")
                if top and top[0][0] >= thr:
                    st.warning("üö® –ï—Å—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç —Å –ø–æ—Ö–æ–∂–µ—Å—Ç—å—é –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞!")

if __name__ == "__main__":
    main()
